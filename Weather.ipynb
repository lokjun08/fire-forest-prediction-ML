{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb61cd9-7cc1-4243-8ae7-745e0131463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sheets: ['BORNEO(MY)_DATA']...\n",
      " Reading BORNEO(MY)_DATA...\n",
      "✅ Combined Total Rows: 8545\n",
      "Processing 8545 rows...\n",
      "\n",
      "Start Fetching Data...\n",
      "  Processed 0 / 8545 rows...\n",
      "  Processed 5 / 8545 rows...\n",
      "  Processed 10 / 8545 rows...\n",
      "  Processed 15 / 8545 rows...\n",
      "  Processed 20 / 8545 rows...\n",
      "  Processed 25 / 8545 rows...\n",
      "  Processed 30 / 8545 rows...\n",
      "  Processed 35 / 8545 rows...\n",
      "  Processed 40 / 8545 rows...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "\n",
    "excel_filename = 'ML DATASET.xlsx'\n",
    "target_sheets = ['BORNEO(MY)_DATA']\n",
    "OUTPUT_FILE = 'final_borneo_malaysia_data.csv' \n",
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "print(f\"Loading sheets: {target_sheets}...\")\n",
    "df_list = []\n",
    "\n",
    "try:\n",
    "    for sheet in target_sheets:\n",
    "        print(f\" Reading {sheet}...\")\n",
    "        df_sheet = pd.read_excel(excel_filename, sheet_name=sheet)\n",
    "        df_list.append(df_sheet)\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"✅ Combined Total Rows: {len(df)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading Excel: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- sample size ---\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    print(f\"⚠️ TEST MODE: Sampling {SAMPLE_SIZE} rows only...\")\n",
    "    df_final = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "else:\n",
    "    df_final = df.copy()\n",
    "\n",
    "print(f\"Processing {len(df_final)} rows...\")\n",
    "\n",
    "# --- data ---\n",
    "def get_raw_7days_weather(lat, lon, fire_date_str):\n",
    "    try:\n",
    "        if isinstance(fire_date_str, pd.Timestamp):\n",
    "            date_obj = fire_date_str\n",
    "        else:\n",
    "            date_obj = datetime.strptime(str(fire_date_str).split(' ')[0], '%Y-%m-%d')\n",
    "\n",
    "        date_keys = []\n",
    "        for i in range(7, -1, -1): \n",
    "            d = date_obj - timedelta(days=i)\n",
    "            date_keys.append(d.strftime('%Y%m%d'))\n",
    "\n",
    "        start_str = date_keys[0]  \n",
    "        end_str = date_keys[-1]   \n",
    "        \n",
    "        base_url = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
    "        params = {\n",
    "            'parameters': 'T2M,PRECTOTCORR,RH2M,WS2M,GWETTOP,GWETROOT', \n",
    "            'community': 'AG',\n",
    "            'longitude': lon,\n",
    "            'latitude': lat,\n",
    "            'start': start_str,\n",
    "            'end': end_str,\n",
    "            'format': 'JSON'\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params, timeout=25)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            p = data['properties']['parameter']\n",
    "            row_weather = {}\n",
    "\n",
    "            for i, date_key in enumerate(date_keys):\n",
    "                days_ago = 7 - i \n",
    "                \n",
    "                val_t = p['T2M'].get(date_key, -999)\n",
    "                row_weather[f'Temp_Day_{days_ago}'] = val_t if val_t > -100 else None\n",
    "                \n",
    "                val_r = p['PRECTOTCORR'].get(date_key, -999)\n",
    "                row_weather[f'Rain_Day_{days_ago}'] = val_r if val_r >= 0 else None\n",
    "                \n",
    "                val_h = p['RH2M'].get(date_key, -999)\n",
    "                row_weather[f'Humid_Day_{days_ago}'] = val_h if val_h >= 0 else None\n",
    "\n",
    "                val_w = p['WS2M'].get(date_key, -999)\n",
    "                row_weather[f'Wind_Day_{days_ago}'] = val_w if val_w >= 0 else None\n",
    "\n",
    "                val_st = p['GWETTOP'].get(date_key, -999)\n",
    "                row_weather[f'SoilSurf_Day_{days_ago}'] = val_st if val_st >= 0 else None\n",
    "\n",
    "                val_sr = p['GWETROOT'].get(date_key, -999)\n",
    "                row_weather[f'SoilRoot_Day_{days_ago}'] = val_sr if val_sr >= 0 else None\n",
    "                \n",
    "            return row_weather\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- repeat ---\n",
    "print(\"\\nStart Fetching Data...\")\n",
    "weather_results = [] \n",
    "\n",
    "for index, row in df_final.iterrows():\n",
    "    if len(weather_results) % 5 == 0:\n",
    "        print(f\"  Processed {len(weather_results)} / {len(df_final)} rows...\")\n",
    "\n",
    "    w_data = get_raw_7days_weather(row['latitude'], row['longitude'], row['acq_date'])\n",
    "    \n",
    "    if w_data:\n",
    "        weather_results.append(w_data)\n",
    "    else:\n",
    "        weather_results.append({})\n",
    "    \n",
    "    time.sleep(0.3)\n",
    "\n",
    "# --- excel ---\n",
    "print(\"Merging and Grouping Columns...\")\n",
    "weather_df = pd.DataFrame(weather_results)\n",
    "\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "weather_df = weather_df.reset_index(drop=True)\n",
    "df_final = pd.concat([df_final, weather_df], axis=1)\n",
    "\n",
    "\n",
    "feature_groups = ['Temp', 'Rain', 'Humid', 'Wind', 'SoilSurf', 'SoilRoot']\n",
    "desired_weather_order = []\n",
    "\n",
    "for feature in feature_groups:\n",
    "    for day in range(7, -1, -1):\n",
    "        col_name = f'{feature}_Day_{day}'\n",
    "        desired_weather_order.append(col_name)\n",
    "\n",
    "original_cols = [col for col in df_final.columns if col not in desired_weather_order]\n",
    "\n",
    "existing_weather_cols = [c for c in desired_weather_order if c in df_final.columns]\n",
    "\n",
    "final_order = original_cols + existing_weather_cols\n",
    "df_final = df_final[final_order]\n",
    "\n",
    "if 'Temp_Day_0' in df_final.columns:\n",
    "    missing_count = df_final['Temp_Day_0'].isna().sum()\n",
    "    print(f\"Note: {missing_count} rows have missing weather data (Kept as empty cells).\")\n",
    "\n",
    "df_final.to_csv(OUTPUT_FILE, index=False, na_rep='')\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"DONE! Saved to: {OUTPUT_FILE}\")\n",
    "print(\"Columns are grouped by type: All Temps -> All Rains -> All Winds...\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b2846-82b1-4f54-8498-7b88c1714d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d4d39-daae-493f-a8e4-d5573ce32cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
